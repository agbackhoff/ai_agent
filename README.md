# AI Schema Converter para DBT

Un agente de inteligencia artificial que convierte schemas de BigQuery y diccionarios de datos en archivos modelo de DBT (`.yml`).

## ğŸš€ CaracterÃ­sticas

- Procesa schemas de BigQuery en formato TXT
- Analiza diccionarios de datos en formato TXT
- Genera archivos modelo DBT (`.yml`) automÃ¡ticamente
- Utiliza Google Gemini API para procesamiento inteligente
- Implementado con PydanticAI para validaciÃ³n de datos
- Containerizado con Docker para fÃ¡cil despliegue
- Logging detallado para debugging y monitoreo
- Manejo robusto de errores y fallbacks

## ğŸ“‹ Requisitos Previos

- Python 3.9+
- Docker y Docker Compose
- API Key de Google Gemini
- Archivos de entrada:
  - Schema de BigQuery (`.txt`)
  - Diccionario de datos (`.txt`)

## ğŸ› ï¸ InstalaciÃ³n

1. Clona el repositorio:
```bash
git clone [url-del-repositorio]
cd ai_schema_converter
```

2. Crea un archivo `.env` con tus credenciales:
```
GEMINI_API_KEY=tu_api_key_aqui
```

3. Ejecuta el proyecto con Docker Compose:
```bash
docker compose up --build
```

## ğŸ“ Estado Actual del Desarrollo

### âœ… Completado

1. **ConfiguraciÃ³n del Proyecto**
   - Estructura bÃ¡sica del proyecto implementada
   - Docker y Docker Compose configurados
   - Sistema de logging detallado implementado
   - GestiÃ³n de variables de entorno configurada

2. **IntegraciÃ³n con Gemini AI**
   - Cliente de Gemini AI configurado
   - Sistema de prompts bÃ¡sico implementado
   - Manejo de respuestas JSON implementado
   - Sistema de fallback para errores

3. **ValidaciÃ³n de Datos**
   - Modelos Pydantic para validaciÃ³n implementados
   - Manejo de errores robusto
   - Limpieza y parsing de respuestas JSON

### ğŸš§ En Progreso

4. **Desarrollo del Parser de Entrada**
   - [ ] Parser para schemas de BigQuery
   - [ ] Parser para diccionarios de datos
   - [ ] ValidaciÃ³n de formatos de entrada

5. **GeneraciÃ³n de Modelos DBT**
   - [ ] Templates para modelos DBT
   - [ ] LÃ³gica de transformaciÃ³n
   - [ ] ValidaciÃ³n de output

6. **Testing y ValidaciÃ³n**
   - [ ] Tests unitarios
   - [ ] Tests de integraciÃ³n
   - [ ] ValidaciÃ³n de formatos de salida

## ğŸ“ Estructura del Proyecto

```
.
â”œâ”€â”€ Dockerfile              # ConfiguraciÃ³n de Docker
â”œâ”€â”€ docker-compose.yml      # OrquestaciÃ³n de contenedores
â”œâ”€â”€ requirements.txt        # Dependencias de Python
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py            # Punto de entrada principal
â”‚   â”œâ”€â”€ parsers/           # Parsers para entrada de datos
â”‚   â”œâ”€â”€ models/            # Modelos Pydantic
â”‚   â””â”€â”€ utils/             # Utilidades y helpers
â”œâ”€â”€ tests/                 # Tests unitarios y de integraciÃ³n
â”œâ”€â”€ input/                 # Directorio para archivos de entrada
â””â”€â”€ output/                # Directorio para archivos generados
```

## ğŸ”§ Uso Actual

1. AsegÃºrate de tener una API key vÃ¡lida de Google Gemini en tu archivo `.env`

2. Ejecuta el contenedor:
```bash
docker compose up --build
```

3. El sistema mostrarÃ¡ logs detallados de su ejecuciÃ³n y el estado del proceso.

## ğŸ“š Dependencias Principales

```
python-dotenv==1.0.0
pydantic==2.5.2
google-generativeai==0.3.1
asyncio==3.4.3
```

## ğŸ“„ Licencia

[Tipo de Licencia]

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor, abre un issue primero para discutir los cambios que te gustarÃ­a realizar.

## ğŸ”œ PrÃ³ximos Pasos

1. Implementar el parser de schemas de BigQuery
2. Desarrollar el parser de diccionarios de datos
3. Crear los templates base para modelos DBT
4. Implementar la lÃ³gica de transformaciÃ³n
5. Agregar tests unitarios y de integraciÃ³n